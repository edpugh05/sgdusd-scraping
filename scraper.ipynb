{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scraping\n",
      "collecting high priority indicators\n",
      "collecting medium priority indicators\n",
      "collecting market levels\n",
      "collecting currency rates\n",
      "collecting Singapore stock prices\n",
      "collecting commodity indicators\n",
      "collecting economic indicators\n",
      "\n",
      "================================================================================\n",
      "COMPREHENSIVE SGD/USD MARKET DATA\n",
      "================================================================================\n",
      "SGDUSD rate: 0.7783\n",
      "USDSGD rate: 1.2848\n",
      "daily_change_pct: +0.08\n",
      "high_52w: 0.7876\n",
      "low_52w: 0.7061\n",
      "SG1Y: 1.770\n",
      "SG2Y: 1.671\n",
      "SG5Y: 1.751\n",
      "SG10Y: 2.086\n",
      "US2Y: 3.863\n",
      "US5Y: 3.948\n",
      "US10Y: 4.421\n",
      "US30Y: 4.995\n",
      "VIX: 16.60\n",
      "DXY: 98.20\n",
      "Gold: 3358.45\n",
      "Oil: 67.42\n",
      "MSCI_Asia: 204.28\n",
      "sp500_change_pct: -0.11\n",
      "gold_change_pct: +0.40\n",
      "singapore_sti_change_pct: +0.76\n",
      "nikkei_change_pct: -0.21\n",
      "hang_seng_change_pct: +1.33\n",
      "oil_wti_change_pct: -0.27\n",
      "dbs_change_pct: +0.22\n",
      "sg_gdp_growth_yoy: 3.90\n",
      "us_gdp_growth_yoy: -0.50\n",
      "sg_inflation_yoy: 0.90\n",
      "us_inflation_yoy: 2.95\n",
      "weekly_change_pct: N/A\n",
      "monthly_change_pct: +0.13\n",
      "usd_5y_yield: 3.948\n",
      "eur_usd_change_pct: +0.25\n",
      "usd_jpy_change_pct: +0.11\n",
      "vix_change_pct: +0.79\n",
      "dxy_change_pct: -0.27\n",
      "sg_unemployment_rate: 2.10\n",
      "sg_exports_growth: +17.50\n",
      "uob_change_pct: +0.22\n",
      "dbs_change_pct: +0.22\n",
      "sg_reit_change_pct: N/A\n",
      "sp500_level: 6292.92\n",
      "vix_level: 16.60\n",
      "dxy_level: 98.20\n",
      "nikkei_level: 39819.11\n",
      "hang_seng_level: 24825.66\n",
      "singapore_sti_level: 4189.50\n",
      "gold_price: 3358.45\n",
      "oil_wti_price: 67.36\n",
      "eur_usd_rate: 1.1626\n",
      "usd_jpy_rate: 148.7600\n",
      "dbs_stock_price: 48.79\n",
      "uob_stock_price: 32.87\n",
      "sg_reit_index: 27.24\n",
      "baltic_dry_index: 2030.00\n",
      "palm_oil_price: 4292.00\n",
      "baltic_dry_change_pct: +6.51\n",
      "palm_oil_change_pct: +1.95\n",
      "sg_industrial_production: 5.90\n",
      "us_core_inflation_yoy: 2.80\n",
      "us_nonfarm_payrolls: 159724\n",
      "fed_funds_rate: 4.33\n",
      "mas_policy_rate: N/A\n",
      "68\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import json\n",
    "from datetime import datetime, timedelta\n",
    "import pandas as pd\n",
    "import warnings\n",
    "import time\n",
    "import math\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "load_dotenv('config.env')\n",
    "\n",
    "class ComprehensiveSGDUSDScrapers:\n",
    "    def __init__(self, fred_api_key=None):\n",
    "        self.fred_api_key = fred_api_key\n",
    "        self.headers = {\n",
    "            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',\n",
    "            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',\n",
    "            'Accept-Language': 'en-US,en;q=0.5',\n",
    "            'Accept-Encoding': 'gzip, deflate',\n",
    "            'Connection': 'keep-alive',\n",
    "            'Upgrade-Insecure-Requests': '1',\n",
    "        }\n",
    "\n",
    "    def scrape_bond_yield(self, url, bond_name):\n",
    "        try:\n",
    "            response = requests.get(url, headers=self.headers, timeout=15)\n",
    "            response.raise_for_status()\n",
    "            \n",
    "            soup = BeautifulSoup(response.content, 'html.parser')\n",
    "            \n",
    "            yield_selectors = [\n",
    "                '[data-test=\"instrument-price-last\"]',\n",
    "                '.text-2xl',\n",
    "                '.instrument-price_last__KQzyA',\n",
    "                '.last-price-value',\n",
    "                '.text-5xl',\n",
    "                '.pid-23705-last',\n",
    "                '[data-symbol] [data-field=\"regularMarketPrice\"]'\n",
    "            ]\n",
    "            \n",
    "            for selector in yield_selectors:\n",
    "                element = soup.select_one(selector)\n",
    "                if element:\n",
    "                    text = element.get_text(strip=True)\n",
    "                    yield_match = re.search(r'\\d+\\.\\d{2,3}', text)\n",
    "                    if yield_match:\n",
    "                        potential_yield = float(yield_match.group())\n",
    "                        if 0.1 <= potential_yield <= 15.0:\n",
    "                            return potential_yield\n",
    "            \n",
    "            main_price_elements = soup.find_all(['span', 'div'], \n",
    "                class_=re.compile(r'(price|last|current|main|big|large)', re.I))\n",
    "            \n",
    "            for element in main_price_elements:\n",
    "                text = element.get_text(strip=True)\n",
    "                if re.search(r'\\b\\d+\\.\\d{2,3}%?\\b', text):\n",
    "                    match = re.search(r'\\d+\\.\\d{2,3}', text)\n",
    "                    if match:\n",
    "                        potential_yield = float(match.group())\n",
    "                        if 0.1 <= potential_yield <= 15.0:\n",
    "                            return potential_yield\n",
    "            \n",
    "            for element in soup.find_all(['td', 'th', 'span', 'div', 'strong']):\n",
    "                text = element.get_text(strip=True)\n",
    "                pattern = r'^\\d+\\.\\d{2,3}%?$'\n",
    "                if re.match(pattern, text):\n",
    "                    numeric_value = float(re.sub(r'[^\\d.]', '', text))\n",
    "                    if 0.1 <= numeric_value <= 15.0:\n",
    "                        return numeric_value\n",
    "            \n",
    "            return None\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error scraping {bond_name}: {e}\")\n",
    "            return None\n",
    "\n",
    "    def scrape_percentage_change(self, url, name, min_range=-30, max_range=30):\n",
    "        try:\n",
    "            response = requests.get(url, headers=self.headers, timeout=10)\n",
    "            soup = BeautifulSoup(response.content, 'html.parser')\n",
    "            \n",
    "            change_selectors = [\n",
    "                '[data-test=\"instrument-price-change-percent\"]',\n",
    "                '.text-red-500', '.text-green-500',\n",
    "                '.changePercent', '.change-percent'\n",
    "            ]\n",
    "            \n",
    "            for selector in change_selectors:\n",
    "                element = soup.select_one(selector)\n",
    "                if element:\n",
    "                    text = element.get_text(strip=True)\n",
    "                    pct_match = re.search(r'([+-]?\\d+\\.\\d+)%', text)\n",
    "                    if pct_match:\n",
    "                        change_pct = float(pct_match.group(1))\n",
    "                        if min_range <= change_pct <= max_range:\n",
    "                            return change_pct\n",
    "            \n",
    "            for element in soup.find_all(['span', 'div'], class_=re.compile(r'change|pct', re.I)):\n",
    "                text = element.get_text(strip=True)\n",
    "                pct_match = re.search(r'([+-]?\\d+\\.\\d+)%', text)\n",
    "                if pct_match:\n",
    "                    change_pct = float(pct_match.group(1))\n",
    "                    if min_range <= change_pct <= max_range:\n",
    "                        return change_pct\n",
    "            \n",
    "            for row in soup.find_all('tr'):\n",
    "                cells = row.find_all(['td', 'th'])\n",
    "                if len(cells) >= 2:\n",
    "                    label = cells[0].get_text(strip=True).lower()\n",
    "                    value = cells[1].get_text(strip=True)\n",
    "                    if 'change' in label and '%' in value:\n",
    "                        pct_match = re.search(r'([+-]?\\d+\\.\\d+)%', value)\n",
    "                        if pct_match:\n",
    "                            change_pct = float(pct_match.group(1))\n",
    "                            if min_range <= change_pct <= max_range:\n",
    "                                return change_pct\n",
    "            \n",
    "            return None\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error scraping {name} change: {e}\")\n",
    "            return None\n",
    "\n",
    "    def scrape_marketwatch_change_pct(self, url, name, min_range=-20, max_range=20):\n",
    "        try:\n",
    "            response = requests.get(url, headers=self.headers, timeout=15)\n",
    "            response.raise_for_status()\n",
    "            soup = BeautifulSoup(response.content, 'html.parser')\n",
    "            \n",
    "            percent_elements = soup.find_all(class_=re.compile(r'percent', re.I))\n",
    "            for element in percent_elements:\n",
    "                text = element.get_text(strip=True)\n",
    "                if '%' in text:\n",
    "                    pct_match = re.search(r'([+-]?\\d+\\.\\d+)%', text)\n",
    "                    if pct_match:\n",
    "                        change_pct = float(pct_match.group(1))\n",
    "                        if min_range <= change_pct <= max_range:\n",
    "                            return change_pct\n",
    "            \n",
    "            change_selectors = [\n",
    "                '[class*=\"change\"]',\n",
    "                '[class*=\"Change\"]', \n",
    "                '.pct-change',\n",
    "                '.price-change',\n",
    "                '[data-module*=\"change\"]'\n",
    "            ]\n",
    "            \n",
    "            for selector in change_selectors:\n",
    "                elements = soup.select(selector)\n",
    "                for element in elements:\n",
    "                    text = element.get_text(strip=True)\n",
    "                    if '%' in text:\n",
    "                        pct_match = re.search(r'([+-]?\\d+\\.\\d+)%', text)\n",
    "                        if pct_match:\n",
    "                            change_pct = float(pct_match.group(1))\n",
    "                            if min_range <= change_pct <= max_range:\n",
    "                                return change_pct\n",
    "            \n",
    "            tables = soup.find_all('table')\n",
    "            for table in tables:\n",
    "                rows = table.find_all('tr')\n",
    "                for row in rows:\n",
    "                    cells = row.find_all(['td', 'th'])\n",
    "                    for cell in cells:\n",
    "                        text = cell.get_text(strip=True)\n",
    "                        if '%' in text and len(text) < 20:\n",
    "                            pct_match = re.search(r'([+-]?\\d+\\.\\d+)%', text)\n",
    "                            if pct_match:\n",
    "                                change_pct = float(pct_match.group(1))\n",
    "                                if min_range <= change_pct <= max_range:\n",
    "                                    return change_pct\n",
    "            \n",
    "            for element in soup.find_all(['span', 'div']):\n",
    "                text = element.get_text(strip=True)\n",
    "                if re.match(r'^[+-]?\\d+\\.\\d+%$', text):\n",
    "                    pct_match = re.search(r'([+-]?\\d+\\.\\d+)', text)\n",
    "                    if pct_match:\n",
    "                        change_pct = float(pct_match.group(1))\n",
    "                        if min_range <= change_pct <= max_range:\n",
    "                            return change_pct\n",
    "            \n",
    "            return None\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error scraping {name} change from MarketWatch: {e}\")\n",
    "            return None\n",
    "\n",
    "    def scrape_single_value(self, url, name, min_val, max_val):\n",
    "        try:\n",
    "            response = requests.get(url, headers=self.headers, timeout=10)\n",
    "            soup = BeautifulSoup(response.content, 'html.parser')\n",
    "            \n",
    "            price_selectors = [\n",
    "                '[data-test=\"instrument-price-last\"]',\n",
    "                '.text-2xl', '.text-5xl',\n",
    "                '.instrument-price_last__KQzyA',\n",
    "                '.last-price-value'\n",
    "            ]\n",
    "            \n",
    "            for selector in price_selectors:\n",
    "                element = soup.select_one(selector)\n",
    "                if element:\n",
    "                    text = element.get_text(strip=True)\n",
    "                    clean_text = re.sub(r'[,\\s]', '', text)\n",
    "                    match = re.search(r'\\d+\\.?\\d*', clean_text)\n",
    "                    if match:\n",
    "                        value = float(match.group())\n",
    "                        if min_val <= value <= max_val:\n",
    "                            return value\n",
    "            \n",
    "            for element in soup.find_all(['span', 'div'], class_=re.compile(r'price|last|current', re.I)):\n",
    "                text = element.get_text(strip=True)\n",
    "                clean_text = re.sub(r'[,\\s]', '', text)\n",
    "                match = re.search(r'\\d+\\.?\\d*', clean_text)\n",
    "                if match:\n",
    "                    value = float(match.group())\n",
    "                    if min_val <= value <= max_val:\n",
    "                        return value\n",
    "            \n",
    "            return None\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error scraping {name}: {e}\")\n",
    "            return None\n",
    "\n",
    "    def scrape_single_value_targeted(self, url, name, min_val, max_val):\n",
    "        try:\n",
    "            response = requests.get(url, headers=self.headers, timeout=15)\n",
    "            soup = BeautifulSoup(response.content, 'html.parser')\n",
    "            \n",
    "            element = soup.find('div', {'data-test': 'instrument-price-last'})\n",
    "            if element:\n",
    "                text = element.get_text(strip=True)\n",
    "                clean_text = text.replace(',', '')\n",
    "                if re.match(r'^\\d{4}\\.\\d{2}$', clean_text):\n",
    "                    value = float(clean_text)\n",
    "                    if min_val <= value <= max_val:\n",
    "                        return value\n",
    "            \n",
    "            return self.scrape_single_value(url, name, min_val, max_val)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error scraping {name}: {e}\")\n",
    "            return None\n",
    "\n",
    "    def scrape_marketwatch_value(self, url, name, min_val, max_val):\n",
    "        try:\n",
    "            response = requests.get(url, headers=self.headers, timeout=15)\n",
    "            soup = BeautifulSoup(response.content, 'html.parser')\n",
    "            \n",
    "            percent_elements = soup.find_all(class_=re.compile(r'percent', re.I))\n",
    "            for element in percent_elements:\n",
    "                text = element.get_text(strip=True)\n",
    "                clean_text = re.sub(r'[,\\s]', '', text)\n",
    "                match = re.search(r'\\d+\\.?\\d*', clean_text)\n",
    "                if match:\n",
    "                    value = float(match.group())\n",
    "                    if min_val <= value <= max_val:\n",
    "                        return value\n",
    "            \n",
    "            change_selectors = [\n",
    "                '[class*=\"change\"]',\n",
    "                '[class*=\"Change\"]', \n",
    "                '.pct-change',\n",
    "                '.price-change',\n",
    "                '[class*=\"price\"]',\n",
    "                '[class*=\"last\"]',\n",
    "                '[class*=\"current\"]',\n",
    "                'span.value'\n",
    "            ]\n",
    "            \n",
    "            for selector in change_selectors:\n",
    "                elements = soup.select(selector)\n",
    "                for element in elements:\n",
    "                    text = element.get_text(strip=True)\n",
    "                    clean_text = re.sub(r'[,\\s]', '', text)\n",
    "                    match = re.search(r'\\d+\\.?\\d*', clean_text)\n",
    "                    if match:\n",
    "                        value = float(match.group())\n",
    "                        if min_val <= value <= max_val:\n",
    "                            return value\n",
    "            \n",
    "            tables = soup.find_all('table')\n",
    "            for table in tables:\n",
    "                rows = table.find_all('tr')\n",
    "                for row in rows:\n",
    "                    cells = row.find_all(['td', 'th'])\n",
    "                    for cell in cells:\n",
    "                        text = cell.get_text(strip=True)\n",
    "                        clean_text = re.sub(r'[,\\s]', '', text)\n",
    "                        match = re.search(r'\\d+\\.?\\d*', clean_text)\n",
    "                        if match:\n",
    "                            value = float(match.group())\n",
    "                            if min_val <= value <= max_val:\n",
    "                                return value\n",
    "            \n",
    "            for element in soup.find_all(['span', 'div']):\n",
    "                text = element.get_text(strip=True)\n",
    "                clean_text = re.sub(r'[,\\s]', '', text)\n",
    "                \n",
    "                if name == \"STI\":\n",
    "                    if re.match(r'^\\d{4}\\.\\d{2}$', clean_text):\n",
    "                        value = float(clean_text)\n",
    "                        if min_val <= value <= max_val:\n",
    "                            return value\n",
    "                elif \"stock\" in name.lower():\n",
    "                    if re.match(r'^\\d{2}\\.\\d{2}$', clean_text):\n",
    "                        value = float(clean_text)\n",
    "                        if min_val <= value <= max_val:\n",
    "                            return value\n",
    "                else:\n",
    "                    match = re.search(r'\\d+\\.?\\d*', clean_text)\n",
    "                    if match:\n",
    "                        value = float(match.group())\n",
    "                        if min_val <= value <= max_val:\n",
    "                            return value\n",
    "            \n",
    "            return None\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error scraping {name} from MarketWatch: {e}\")\n",
    "            return None\n",
    "\n",
    "    def scrape_fred_data(self, series_id, limit=1):\n",
    "        if not self.fred_api_key:\n",
    "            return None\n",
    "            \n",
    "        try:\n",
    "            url = f\"https://api.stlouisfed.org/fred/series/observations\"\n",
    "            params = {\n",
    "                'series_id': series_id,\n",
    "                'api_key': self.fred_api_key,\n",
    "                'file_type': 'json',\n",
    "                'limit': limit,\n",
    "                'sort_order': 'desc'\n",
    "            }\n",
    "            \n",
    "            response = requests.get(url, params=params, timeout=10)\n",
    "            data = response.json()\n",
    "            \n",
    "            if 'observations' in data and data['observations']:\n",
    "                latest = data['observations'][0]\n",
    "                return {\n",
    "                    'value': float(latest['value']) if latest['value'] != '.' else None,\n",
    "                    'date': latest['date']\n",
    "                }\n",
    "            return None\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error getting FRED data for {series_id}: {e}\")\n",
    "            return None\n",
    "\n",
    "    def scrape_singapore_1y_bond(self):\n",
    "        url = 'https://www.investing.com/rates-bonds/singapore-1-year-bond-yield'\n",
    "        return self.scrape_bond_yield(url, \"Singapore 1Y\")\n",
    "\n",
    "    def scrape_singapore_2y_bond(self):\n",
    "        url = 'https://www.investing.com/rates-bonds/singapore-2-year-bond-yield'\n",
    "        return self.scrape_bond_yield(url, \"Singapore 2Y\")\n",
    "\n",
    "    def scrape_singapore_5y_bond(self):\n",
    "        url = 'https://www.investing.com/rates-bonds/singapore-5-year-bond-yield'\n",
    "        return self.scrape_bond_yield(url, \"Singapore 5Y\")\n",
    "\n",
    "    def scrape_singapore_10y_bond(self):\n",
    "        url = 'https://www.investing.com/rates-bonds/singapore-10-year-bond-yield'\n",
    "        return self.scrape_bond_yield(url, \"Singapore 10Y\")\n",
    "\n",
    "    def scrape_us_2y_treasury(self):\n",
    "        url = 'https://www.investing.com/rates-bonds/u.s.-2-year-bond-yield'\n",
    "        return self.scrape_bond_yield(url, \"US 2Y\")\n",
    "\n",
    "    def scrape_us_5y_treasury(self):\n",
    "        try:\n",
    "            url = 'https://www.investing.com/rates-bonds/u.s.-5-year-bond-yield'\n",
    "            response = requests.get(url, headers=self.headers, timeout=15)\n",
    "            soup = BeautifulSoup(response.content, 'html.parser')\n",
    "            \n",
    "            yield_selectors = [\n",
    "                '[data-test=\"instrument-price-last\"]',\n",
    "                '.text-2xl',\n",
    "                '.instrument-price_last__KQzyA',\n",
    "                '.last-price-value',\n",
    "                '.text-5xl'\n",
    "            ]\n",
    "            \n",
    "            for selector in yield_selectors:\n",
    "                element = soup.select_one(selector)\n",
    "                if element:\n",
    "                    text = element.get_text(strip=True)\n",
    "                    yield_match = re.search(r'\\d+\\.\\d{2,3}', text)\n",
    "                    if yield_match:\n",
    "                        potential_yield = float(yield_match.group())\n",
    "                        if 0.1 <= potential_yield <= 15.0:\n",
    "                            return potential_yield\n",
    "            \n",
    "            return None\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error scraping US 5Y yield: {e}\")\n",
    "            return None\n",
    "\n",
    "    def scrape_us_10y_treasury(self):\n",
    "        url = 'https://www.investing.com/rates-bonds/u.s.-10-year-bond-yield'\n",
    "        return self.scrape_bond_yield(url, \"US 10Y\")\n",
    "\n",
    "    def scrape_us_30y_treasury(self):\n",
    "        url = 'https://www.investing.com/rates-bonds/u.s.-30-year-bond-yield'\n",
    "        return self.scrape_bond_yield(url, \"US 30Y\")\n",
    "\n",
    "    def scrape_sgd_usd_current_rate(self):\n",
    "        try:\n",
    "            url = \"https://www.investing.com/currencies/sgd-usd\"\n",
    "            response = requests.get(url, headers=self.headers, timeout=10)\n",
    "            soup = BeautifulSoup(response.content, 'html.parser')\n",
    "            \n",
    "            for element in soup.find_all(['span', 'div'], class_=re.compile(r'price|last|current', re.I)):\n",
    "                text = element.get_text(strip=True)\n",
    "                pattern = r'^0\\.\\d{4}$'\n",
    "                if re.match(pattern, text):\n",
    "                    rate_val = float(text)\n",
    "                    if 0.65 <= rate_val <= 0.85:\n",
    "                        return rate_val\n",
    "            \n",
    "            url = \"https://tradingeconomics.com/singapore/currency\"\n",
    "            response = requests.get(url, headers=self.headers, timeout=10)\n",
    "            soup = BeautifulSoup(response.content, 'html.parser')\n",
    "            \n",
    "            for element in soup.find_all(['span', 'div', 'td']):\n",
    "                text = element.get_text(strip=True)\n",
    "                if re.search(r'1\\.[0-9]{4}', text):\n",
    "                    rate_match = re.search(r'1\\.\\d{4}', text)\n",
    "                    if rate_match:\n",
    "                        rate = float(rate_match.group())\n",
    "                        if 1.2 <= rate <= 1.5:\n",
    "                            return 1 / rate\n",
    "            \n",
    "            return None\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error getting SGD/USD rate: {e}\")\n",
    "            return None\n",
    "\n",
    "    def scrape_sgd_usd_daily_change_pct(self):\n",
    "        url = \"https://www.investing.com/currencies/sgd-usd\"\n",
    "        return self.scrape_percentage_change(url, \"SGD/USD daily change\", -10, 10)\n",
    "\n",
    "    def scrape_sgd_usd_weekly_change_pct(self):\n",
    "        try:\n",
    "            url = \"https://finance.yahoo.com/quote/SGDUSD=X/\"\n",
    "            response = requests.get(url, headers=self.headers, timeout=15)\n",
    "            soup = BeautifulSoup(response.content, 'html.parser')\n",
    "            \n",
    "            for table in soup.find_all('table'):\n",
    "                for row in table.find_all('tr'):\n",
    "                    cells = row.find_all(['td', 'th'])\n",
    "                    if len(cells) >= 2:\n",
    "                        label = cells[0].get_text(strip=True).lower()\n",
    "                        value = cells[1].get_text(strip=True)\n",
    "                        \n",
    "                        if any(term in label for term in ['week', '1w', '7d']) and '%' in value:\n",
    "                            pct_match = re.search(r'([+-]?\\d+\\.\\d+)%', value)\n",
    "                            if pct_match:\n",
    "                                change_pct = float(pct_match.group(1))\n",
    "                                if -20 <= change_pct <= 20:\n",
    "                                    return change_pct\n",
    "            \n",
    "            for element in soup.find_all(['span', 'div']):\n",
    "                text = element.get_text(strip=True)\n",
    "                weekly_pattern = r'(week|1w|7d)[^%]*([+-]?\\d+\\.\\d+)%'\n",
    "                match = re.search(weekly_pattern, text, re.IGNORECASE)\n",
    "                if match:\n",
    "                    change_pct = float(match.group(2))\n",
    "                    if -20 <= change_pct <= 20:\n",
    "                        return change_pct\n",
    "            \n",
    "            return None\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error scraping SGD/USD weekly change: {e}\")\n",
    "            return None\n",
    "\n",
    "    def scrape_sgd_usd_monthly_change_pct(self):\n",
    "        try:\n",
    "            url = \"https://finance.yahoo.com/quote/SGDUSD=X/\"\n",
    "            response = requests.get(url, headers=self.headers, timeout=15)\n",
    "            soup = BeautifulSoup(response.content, 'html.parser')\n",
    "            \n",
    "            for table in soup.find_all('table'):\n",
    "                for row in table.find_all('tr'):\n",
    "                    cells = row.find_all(['td', 'th'])\n",
    "                    if len(cells) >= 2:\n",
    "                        label = cells[0].get_text(strip=True).lower()\n",
    "                        value = cells[1].get_text(strip=True)\n",
    "                        \n",
    "                        if any(term in label for term in ['month', '1m', '30d']) and '%' in value:\n",
    "                            pct_match = re.search(r'([+-]?\\d+\\.\\d+)%', value)\n",
    "                            if pct_match:\n",
    "                                change_pct = float(pct_match.group(1))\n",
    "                                if -30 <= change_pct <= 30:\n",
    "                                    return change_pct\n",
    "            \n",
    "            for element in soup.find_all(['span', 'div']):\n",
    "                text = element.get_text(strip=True)\n",
    "                monthly_pattern = r'(month|1m|30d)[^%]*([+-]?\\d+\\.\\d+)%'\n",
    "                match = re.search(monthly_pattern, text, re.IGNORECASE)\n",
    "                if match:\n",
    "                    change_pct = float(match.group(2))\n",
    "                    if -30 <= change_pct <= 30:\n",
    "                        return change_pct\n",
    "            \n",
    "            try:\n",
    "                url = \"https://www.investing.com/currencies/sgd-usd\"\n",
    "                response = requests.get(url, headers=self.headers, timeout=10)\n",
    "                soup = BeautifulSoup(response.content, 'html.parser')\n",
    "                \n",
    "                for row in soup.find_all('tr'):\n",
    "                    cells = row.find_all(['td', 'th'])\n",
    "                    if len(cells) >= 2:\n",
    "                        label = cells[0].get_text(strip=True).lower()\n",
    "                        value = cells[1].get_text(strip=True)\n",
    "                        \n",
    "                        if any(term in label for term in ['month', '1m', '30d']) and '%' in value:\n",
    "                            pct_match = re.search(r'([+-]?\\d+\\.\\d+)%', value)\n",
    "                            if pct_match:\n",
    "                                change_pct = float(pct_match.group(1))\n",
    "                                if -30 <= change_pct <= 30:\n",
    "                                    return change_pct\n",
    "            except:\n",
    "                pass\n",
    "            \n",
    "            try:\n",
    "                url = \"https://tradingeconomics.com/sgdusd:cur\"\n",
    "                response = requests.get(url, headers=self.headers, timeout=10)\n",
    "                soup = BeautifulSoup(response.content, 'html.parser')\n",
    "                \n",
    "                for element in soup.find_all(['span', 'div', 'td']):\n",
    "                    text = element.get_text(strip=True)\n",
    "                    if 'month' in text.lower() and '%' in text:\n",
    "                        pct_match = re.search(r'([+-]?\\d+\\.\\d+)%', text)\n",
    "                        if pct_match:\n",
    "                            change_pct = float(pct_match.group(1))\n",
    "                            if -30 <= change_pct <= 30:\n",
    "                                return change_pct\n",
    "            except:\n",
    "                pass\n",
    "            \n",
    "            return None\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error scraping SGD/USD monthly change: {e}\")\n",
    "            return None\n",
    "\n",
    "    def scrape_sgd_usd_52w_high(self):\n",
    "        try:\n",
    "            url = \"https://www.investing.com/currencies/sgd-usd\"\n",
    "            response = requests.get(url, headers=self.headers, timeout=10)\n",
    "            soup = BeautifulSoup(response.content, 'html.parser')\n",
    "            \n",
    "            for row in soup.find_all('tr'):\n",
    "                cells = row.find_all(['td', 'th'])\n",
    "                if len(cells) >= 2:\n",
    "                    label = cells[0].get_text(strip=True).lower()\n",
    "                    value = cells[1].get_text(strip=True)\n",
    "                    \n",
    "                    if any(term in label for term in ['52', 'week', 'wk']) and 'range' in label:\n",
    "                        range_match = re.findall(r'0\\.\\d{4}', value)\n",
    "                        if len(range_match) == 2:\n",
    "                            return max(float(x) for x in range_match)\n",
    "                    \n",
    "                    elif any(term in label for term in ['52', 'week', 'wk']) and 'high' in label:\n",
    "                        rate_match = re.search(r'0\\.\\d{4}', value)\n",
    "                        if rate_match:\n",
    "                            return float(rate_match.group())\n",
    "            \n",
    "            for element in soup.find_all(['span', 'div']):\n",
    "                if '52' in element.get_text():\n",
    "                    parent = element.find_parent()\n",
    "                    if parent:\n",
    "                        rates = re.findall(r'0\\.\\d{4}', parent.get_text())\n",
    "                        if len(rates) >= 2:\n",
    "                            return max(float(x) for x in rates)\n",
    "            \n",
    "            return None\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error scraping SGD/USD 52w high: {e}\")\n",
    "            return None\n",
    "\n",
    "    def scrape_sgd_usd_52w_low(self):\n",
    "        try:\n",
    "            url = \"https://www.investing.com/currencies/sgd-usd\"\n",
    "            response = requests.get(url, headers=self.headers, timeout=10)\n",
    "            soup = BeautifulSoup(response.content, 'html.parser')\n",
    "            \n",
    "            for row in soup.find_all('tr'):\n",
    "                cells = row.find_all(['td', 'th'])\n",
    "                if len(cells) >= 2:\n",
    "                    label = cells[0].get_text(strip=True).lower()\n",
    "                    value = cells[1].get_text(strip=True)\n",
    "                    \n",
    "                    if any(term in label for term in ['52', 'week', 'wk']) and 'range' in label:\n",
    "                        range_match = re.findall(r'0\\.\\d{4}', value)\n",
    "                        if len(range_match) == 2:\n",
    "                            valid_rates = [float(x) for x in range_match if 0.6 <= float(x) <= 0.9]\n",
    "                            if len(valid_rates) == 2:\n",
    "                                return min(valid_rates)\n",
    "                    \n",
    "                    elif any(term in label for term in ['52', 'week', 'wk']) and 'low' in label:\n",
    "                        rate_match = re.search(r'0\\.\\d{4}', value)\n",
    "                        if rate_match:\n",
    "                            rate_val = float(rate_match.group())\n",
    "                            if 0.6 <= rate_val <= 0.9:\n",
    "                                return rate_val\n",
    "            \n",
    "            for element in soup.find_all(['span', 'div']):\n",
    "                if '52' in element.get_text():\n",
    "                    parent = element.find_parent()\n",
    "                    if parent:\n",
    "                        rates = re.findall(r'0\\.\\d{4}', parent.get_text())\n",
    "                        valid_rates = [float(x) for x in rates if 0.6 <= float(x) <= 0.9]\n",
    "                        if len(valid_rates) >= 2:\n",
    "                            return min(valid_rates)\n",
    "            \n",
    "            return None\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error scraping SGD/USD 52w low: {e}\")\n",
    "            return None\n",
    "\n",
    "    def get_comprehensive_forex_data(self):\n",
    "        try:\n",
    "            url = \"https://www.investing.com/currencies/sgd-usd\"\n",
    "            response = requests.get(url, headers=self.headers, timeout=10)\n",
    "            soup = BeautifulSoup(response.content, 'html.parser')\n",
    "            \n",
    "            current_rate = None\n",
    "            day_high = None\n",
    "            day_low = None\n",
    "            change_pct = None\n",
    "            \n",
    "            for row in soup.find_all('tr'):\n",
    "                cells = row.find_all(['td', 'th'])\n",
    "                if len(cells) >= 2:\n",
    "                    label = cells[0].get_text(strip=True).lower()\n",
    "                    value = cells[1].get_text(strip=True)\n",
    "                    \n",
    "                    if 'high' in label and ('day' in label or 'daily' in label):\n",
    "                        if re.search(r'0\\.\\d{4}', value):\n",
    "                            day_high = float(re.search(r'0\\.\\d{4}', value).group())\n",
    "                    elif 'low' in label and ('day' in label or 'daily' in label):\n",
    "                        if re.search(r'0\\.\\d{4}', value):\n",
    "                            day_low = float(re.search(r'0\\.\\d{4}', value).group())\n",
    "                    elif 'change' in label and '%' in value:\n",
    "                        pct_match = re.search(r'([+-]?\\d+\\.\\d+)%', value)\n",
    "                        if pct_match:\n",
    "                            change_pct = float(pct_match.group(1))\n",
    "            \n",
    "            if not current_rate:\n",
    "                current_rate = self.scrape_sgd_usd_current_rate()\n",
    "            \n",
    "            return {\n",
    "                'current_rate': current_rate,\n",
    "                '24h_high': day_high,\n",
    "                '24h_low': day_low,\n",
    "                'daily_change_pct': change_pct\n",
    "            }\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error getting comprehensive forex data: {e}\")\n",
    "            return {'current_rate': None, '24h_high': None, '24h_low': None, 'daily_change_pct': None}\n",
    "\n",
    "    def scrape_eur_usd_rate(self):\n",
    "        url = \"https://www.investing.com/currencies/eur-usd\"\n",
    "        return self.scrape_single_value(url, \"EUR/USD\", 0.9, 1.3)\n",
    "\n",
    "    def scrape_usd_jpy_rate(self):\n",
    "        url = \"https://www.investing.com/currencies/usd-jpy\"\n",
    "        return self.scrape_single_value(url, \"USD/JPY\", 100, 160)\n",
    "\n",
    "    def scrape_eur_usd_change_pct(self):\n",
    "        url = \"https://www.investing.com/currencies/eur-usd\"\n",
    "        return self.scrape_percentage_change(url, \"EUR/USD change\", -10, 10)\n",
    "\n",
    "    def scrape_usd_jpy_change_pct(self):\n",
    "        url = \"https://www.investing.com/currencies/usd-jpy\"\n",
    "        return self.scrape_percentage_change(url, \"USD/JPY change\", -10, 10)\n",
    "\n",
    "    def scrape_vix(self):\n",
    "        url = \"https://www.investing.com/indices/volatility-s-p-500\"\n",
    "        return self.scrape_single_value(url, \"VIX\", 5, 100)\n",
    "\n",
    "    def scrape_vix_level(self):\n",
    "        url = \"https://www.investing.com/indices/volatility-s-p-500\"\n",
    "        return self.scrape_single_value(url, \"VIX\", 5, 100)\n",
    "\n",
    "    def scrape_vix_change_pct(self):\n",
    "        url = \"https://www.investing.com/indices/volatility-s-p-500\"\n",
    "        return self.scrape_percentage_change(url, \"VIX change\", -50, 50)\n",
    "\n",
    "    def scrape_dxy(self):\n",
    "        url = \"https://www.investing.com/currencies/us-dollar-index\"\n",
    "        return self.scrape_single_value(url, \"DXY\", 70, 130)\n",
    "\n",
    "    def scrape_dxy_level(self):\n",
    "        url = \"https://www.investing.com/currencies/us-dollar-index\"\n",
    "        return self.scrape_single_value(url, \"DXY\", 70, 130)\n",
    "\n",
    "    def scrape_dxy_change_pct(self):\n",
    "        url = \"https://www.investing.com/currencies/us-dollar-index\"\n",
    "        return self.scrape_percentage_change(url, \"DXY change\", -10, 10)\n",
    "\n",
    "    def scrape_gold_price(self):\n",
    "        try:\n",
    "            url = \"https://www.investing.com/commodities/gold\"\n",
    "            response = requests.get(url, headers=self.headers, timeout=10)\n",
    "            soup = BeautifulSoup(response.content, 'html.parser')\n",
    "            \n",
    "            price_selectors = [\n",
    "                '[data-test=\"instrument-price-last\"]',\n",
    "                '.text-2xl', '.text-5xl',\n",
    "                '.instrument-price_last__KQzyA',\n",
    "                '.last-price-value',\n",
    "                'span[class*=\"text-\"]',\n",
    "                'div[class*=\"text-\"]'\n",
    "            ]\n",
    "            \n",
    "            for selector in price_selectors:\n",
    "                elements = soup.select(selector)\n",
    "                for element in elements:\n",
    "                    text = element.get_text(strip=True)\n",
    "                    clean_text = re.sub(r'[,\\s]', '', text)\n",
    "                    pattern = r'\\b(\\d{4})\\.\\d{2}\\b'\n",
    "                    match = re.search(pattern, clean_text)\n",
    "                    if match:\n",
    "                        value = float(clean_text)\n",
    "                        if 1500 <= value <= 5000:\n",
    "                            return value\n",
    "            \n",
    "            for element in soup.find_all(['span', 'div']):\n",
    "                text = element.get_text(strip=True)\n",
    "                pattern = r'^\\d{4}\\.\\d{2}$'\n",
    "                if re.match(pattern, text):\n",
    "                    value = float(text)\n",
    "                    if 1500 <= value <= 5000:\n",
    "                        return value\n",
    "            \n",
    "            return None\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error scraping Gold price: {e}\")\n",
    "            return None\n",
    "\n",
    "    def scrape_gold_change_pct(self):\n",
    "        url = \"https://www.investing.com/commodities/gold\"\n",
    "        return self.scrape_percentage_change(url, \"Gold change\", -10, 10)\n",
    "\n",
    "    def scrape_oil_wti_price(self):\n",
    "        url = \"https://www.investing.com/commodities/crude-oil\"\n",
    "        return self.scrape_single_value(url, \"Oil\", 30, 150)\n",
    "\n",
    "    def scrape_oil_wti_change_pct(self):\n",
    "        url = \"https://www.investing.com/commodities/crude-oil\"\n",
    "        return self.scrape_percentage_change(url, \"Oil WTI change\", -20, 20)\n",
    "\n",
    "    def scrape_msci_asia_pacific(self):\n",
    "        url = \"https://www.investing.com/indices/msci-ac-asia-pacific\"\n",
    "        return self.scrape_single_value(url, \"MSCI Asia\", 100, 300)\n",
    "\n",
    "    def scrape_sp500_level(self):\n",
    "        url = \"https://www.investing.com/indices/us-spx-500\"\n",
    "        return self.scrape_single_value_targeted(url, \"S&P 500\", 3000, 7000)\n",
    "\n",
    "    def scrape_sp500_change_pct(self):\n",
    "        url = \"https://www.investing.com/indices/us-spx-500\"\n",
    "        return self.scrape_percentage_change(url, \"S&P 500 change\", -20, 20)\n",
    "\n",
    "    def scrape_nikkei_level(self):\n",
    "        url = \"https://www.investing.com/indices/japan-ni225\"\n",
    "        return self.scrape_single_value(url, \"Nikkei 225\", 20000, 50000)\n",
    "\n",
    "    def scrape_nikkei_change_pct(self):\n",
    "        url = \"https://www.investing.com/indices/japan-ni225\"\n",
    "        return self.scrape_percentage_change(url, \"Nikkei change\", -15, 15)\n",
    "\n",
    "    def scrape_hang_seng_level(self):\n",
    "        url = \"https://www.investing.com/indices/hang-sen-40\"\n",
    "        return self.scrape_single_value(url, \"Hang Seng\", 15000, 35000)\n",
    "\n",
    "    def scrape_hang_seng_change_pct(self):\n",
    "        url = \"https://www.investing.com/indices/hang-sen-40\"\n",
    "        return self.scrape_percentage_change(url, \"Hang Seng change\", -15, 15)\n",
    "\n",
    "    def scrape_singapore_sti_level(self):\n",
    "        url = \"https://www.marketwatch.com/investing/index/sti?countrycode=sg\"\n",
    "        return self.scrape_marketwatch_value(url, \"STI\", 2500, 5000)\n",
    "\n",
    "    def scrape_singapore_sti_change_pct(self):\n",
    "        url = \"https://www.investing.com/indices/ftse-singapore\"\n",
    "        return self.scrape_percentage_change(url, \"STI change\", -15, 15)\n",
    "\n",
    "    def scrape_dbs_stock_price(self):\n",
    "        url = \"https://www.marketwatch.com/investing/stock/d05?countrycode=sg\"\n",
    "        return self.scrape_marketwatch_value(url, \"DBS stock\", 35, 65)\n",
    "\n",
    "    def scrape_dbs_change_pct(self):\n",
    "        url = \"https://www.marketwatch.com/investing/stock/d05?countrycode=sg\"\n",
    "        return self.scrape_marketwatch_change_pct(url, \"DBS change\", -20, 20)\n",
    "\n",
    "    def scrape_uob_stock_price(self):\n",
    "        url = \"https://www.marketwatch.com/investing/stock/u11?countrycode=sg\"\n",
    "        return self.scrape_marketwatch_value(url, \"UOB stock\", 20, 45)\n",
    "\n",
    "    def scrape_uob_change_pct(self):\n",
    "        try:\n",
    "            url = \"https://www.marketwatch.com/investing/stock/u11?countrycode=sg\"\n",
    "            response = requests.get(url, headers=self.headers, timeout=15)\n",
    "            soup = BeautifulSoup(response.content, 'html.parser')\n",
    "            \n",
    "            percent_elements = soup.find_all(class_=re.compile(r'percent', re.I))\n",
    "            for element in percent_elements:\n",
    "                text = element.get_text(strip=True)\n",
    "                if '%' in text:\n",
    "                    pct_match = re.search(r'([+-]?\\d+\\.\\d+)%', text)\n",
    "                    if pct_match:\n",
    "                        change_pct = float(pct_match.group(1))\n",
    "                        if -20 <= change_pct <= 20:\n",
    "                            return change_pct\n",
    "            \n",
    "            change_selectors = [\n",
    "                '[class*=\"change\"]',\n",
    "                '[class*=\"Change\"]', \n",
    "                '.pct-change',\n",
    "                '.price-change'\n",
    "            ]\n",
    "            \n",
    "            for selector in change_selectors:\n",
    "                elements = soup.select(selector)\n",
    "                for element in elements:\n",
    "                    text = element.get_text(strip=True)\n",
    "                    if '%' in text:\n",
    "                        pct_match = re.search(r'([+-]?\\d+\\.\\d+)%', text)\n",
    "                        if pct_match:\n",
    "                            change_pct = float(pct_match.group(1))\n",
    "                            if -20 <= change_pct <= 20:\n",
    "                                return change_pct\n",
    "            \n",
    "            return None\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error scraping UOB change: {e}\")\n",
    "            return None\n",
    "\n",
    "    def scrape_sg_reit_index(self):\n",
    "        alternative_sources = [\n",
    "            (\"SPDR STI ETF\", \"https://www.investing.com/etfs/spdr-straits-times-index-etf\"),\n",
    "            (\"iShares MSCI Singapore\", \"https://www.investing.com/etfs/ishares-msci-singapore\"),\n",
    "            (\"Lion Phillip S REIT ETF\", \"https://www.investing.com/etfs/lion-phillip-s-reit-etf\"),\n",
    "            (\"Mapletree REIT\", \"https://www.investing.com/equities/mapletree-commercial-trust\")\n",
    "        ]\n",
    "        \n",
    "        for source_name, url in alternative_sources:\n",
    "            try:\n",
    "                response = requests.get(url, headers=self.headers, timeout=10)\n",
    "                if response.status_code == 200:\n",
    "                    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "                    \n",
    "                    element = soup.find('div', {'data-test': 'instrument-price-last'})\n",
    "                    if element:\n",
    "                        text = element.get_text(strip=True)\n",
    "                        clean_text = text.replace(',', '')\n",
    "                        \n",
    "                        if 'etf' in url.lower():\n",
    "                            if re.match(r'^\\d{1,3}\\.\\d{2}$', clean_text):\n",
    "                                value = float(clean_text)\n",
    "                                if 10 <= value <= 200:\n",
    "                                    return value\n",
    "                        else:\n",
    "                            if re.match(r'^\\d\\.\\d{2}$', clean_text):\n",
    "                                value = float(clean_text)\n",
    "                                if 0.5 <= value <= 5.0:\n",
    "                                    return value\n",
    "            except:\n",
    "                continue\n",
    "        \n",
    "        return None\n",
    "\n",
    "    def scrape_sg_reit_change_pct(self):\n",
    "        try:\n",
    "            reit_sources = [\n",
    "                \"https://www.investing.com/indices/ftse-epra-nareit-singapore-capped\",\n",
    "                \"https://www.investing.com/indices/singapore-reit\",\n",
    "                \"https://www.investing.com/etfs/ishares-asia-property-yield\"\n",
    "            ]\n",
    "            \n",
    "            for url in reit_sources:\n",
    "                try:\n",
    "                    response = requests.get(url, headers=self.headers, timeout=10)\n",
    "                    if response.status_code == 200:\n",
    "                        result = self.scrape_percentage_change(url, \"Singapore REIT\", -15, 15)\n",
    "                        if result is not None:\n",
    "                            return result\n",
    "                except:\n",
    "                    continue\n",
    "            \n",
    "            return None\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error scraping SG REIT change: {e}\")\n",
    "            return None\n",
    "\n",
    "    def scrape_baltic_dry_index(self):\n",
    "        try:\n",
    "            url = \"https://www.investing.com/indices/baltic-dry\"\n",
    "            return self.scrape_single_value(url, \"Baltic Dry Index\", 500, 5000)\n",
    "        except Exception as e:\n",
    "            print(f\"Error scraping Baltic Dry Index: {e}\")\n",
    "            return None\n",
    "\n",
    "    def scrape_baltic_dry_change_pct(self):\n",
    "        try:\n",
    "            url = \"https://www.investing.com/indices/baltic-dry\"\n",
    "            response = requests.get(url, headers=self.headers, timeout=10)\n",
    "            soup = BeautifulSoup(response.content, 'html.parser')\n",
    "            \n",
    "            change_selectors = [\n",
    "                '[data-test=\"instrument-price-change-percent\"]',\n",
    "                '.text-red-500', '.text-green-500',\n",
    "                '[class*=\"change\"]', '[class*=\"percent\"]'\n",
    "            ]\n",
    "            \n",
    "            for selector in change_selectors:\n",
    "                elements = soup.select(selector)\n",
    "                for element in elements:\n",
    "                    text = element.get_text(strip=True)\n",
    "                    pct_match = re.search(r'([+-]?\\d+\\.\\d+)%', text)\n",
    "                    if pct_match:\n",
    "                        change_pct = float(pct_match.group(1))\n",
    "                        if -20 <= change_pct <= 20:\n",
    "                            return change_pct\n",
    "            \n",
    "            return None\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error scraping Baltic Dry change: {e}\")\n",
    "            return None\n",
    "\n",
    "    def scrape_palm_oil_price(self):\n",
    "        try:\n",
    "            url = \"https://www.investing.com/commodities/palm-oil\"\n",
    "            return self.scrape_single_value(url, \"Palm Oil\", 2000, 6000)\n",
    "        except Exception as e:\n",
    "            print(f\"Error scraping Palm Oil price: {e}\")\n",
    "            return None\n",
    "\n",
    "    def scrape_palm_oil_change_pct(self):\n",
    "        try:\n",
    "            url = \"https://www.investing.com/commodities/palm-oil\"\n",
    "            response = requests.get(url, headers=self.headers, timeout=10)\n",
    "            soup = BeautifulSoup(response.content, 'html.parser')\n",
    "            \n",
    "            change_selectors = [\n",
    "                '[data-test=\"instrument-price-change-percent\"]',\n",
    "                '.text-red-500', '.text-green-500',\n",
    "                '[class*=\"change\"]', '[class*=\"percent\"]'\n",
    "            ]\n",
    "            \n",
    "            for selector in change_selectors:\n",
    "                elements = soup.select(selector)\n",
    "                for element in elements:\n",
    "                    text = element.get_text(strip=True)\n",
    "                    pct_match = re.search(r'([+-]?\\d+\\.\\d+)%', text)\n",
    "                    if pct_match:\n",
    "                        change_pct = float(pct_match.group(1))\n",
    "                        if -20 <= change_pct <= 20:\n",
    "                            return change_pct\n",
    "            \n",
    "            return None\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error scraping Palm Oil change: {e}\")\n",
    "            return None\n",
    "\n",
    "    def scrape_sgd_usd_volatility_20d(self):\n",
    "        try:\n",
    "            return None\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error calculating SGD/USD 20d volatility: {e}\")\n",
    "            return None\n",
    "\n",
    "    def scrape_sg_gdp_from_trading_economics(self):\n",
    "        try:\n",
    "            url = \"https://tradingeconomics.com/singapore/gdp-growth-annual\"\n",
    "            response = requests.get(url, headers=self.headers, timeout=10)\n",
    "            soup = BeautifulSoup(response.content, 'html.parser')\n",
    "            \n",
    "            for element in soup.find_all(['span', 'div', 'td']):\n",
    "                text = element.get_text(strip=True)\n",
    "                pattern = r'^[+-]?\\d+\\.\\d+%?$'\n",
    "                if re.match(pattern, text):\n",
    "                    numeric_value = float(re.sub(r'[^\\d.-]', '', text))\n",
    "                    if -20 <= numeric_value <= 20:\n",
    "                        return numeric_value\n",
    "            \n",
    "            return None\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error scraping SG GDP growth: {e}\")\n",
    "            return None\n",
    "\n",
    "    def scrape_sg_inflation_from_trading_economics(self):\n",
    "        try:\n",
    "            url = \"https://tradingeconomics.com/singapore/inflation-cpi\"\n",
    "            response = requests.get(url, headers=self.headers, timeout=10)\n",
    "            soup = BeautifulSoup(response.content, 'html.parser')\n",
    "            \n",
    "            for element in soup.find_all(['span', 'div', 'td']):\n",
    "                text = element.get_text(strip=True)\n",
    "                pattern = r'^[+-]?\\d+\\.\\d+%?$'\n",
    "                if re.match(pattern, text):\n",
    "                    numeric_value = float(re.sub(r'[^\\d.-]', '', text))\n",
    "                    if -10 <= numeric_value <= 20:\n",
    "                        return numeric_value\n",
    "            \n",
    "            return None\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error scraping SG inflation: {e}\")\n",
    "            return None\n",
    "\n",
    "    def scrape_sg_unemployment_rate(self):\n",
    "        try:\n",
    "            url = \"https://tradingeconomics.com/singapore/unemployment-rate\"\n",
    "            response = requests.get(url, headers=self.headers, timeout=10)\n",
    "            soup = BeautifulSoup(response.content, 'html.parser')\n",
    "            \n",
    "            for element in soup.find_all(['span', 'div', 'td']):\n",
    "                text = element.get_text(strip=True)\n",
    "                pattern = r'^\\d+\\.\\d+%?$'\n",
    "                if re.match(pattern, text):\n",
    "                    numeric_value = float(re.sub(r'[^\\d.]', '', text))\n",
    "                    if 0.5 <= numeric_value <= 10.0:\n",
    "                        return numeric_value\n",
    "            \n",
    "            return None\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error scraping SG unemployment rate: {e}\")\n",
    "            return None\n",
    "\n",
    "    def scrape_sg_exports_growth(self):\n",
    "        try:\n",
    "            url = \"https://tradingeconomics.com/singapore/exports\"\n",
    "            response = requests.get(url, headers=self.headers, timeout=10)\n",
    "            soup = BeautifulSoup(response.content, 'html.parser')\n",
    "            \n",
    "            for element in soup.find_all(['span', 'div', 'td']):\n",
    "                text = element.get_text(strip=True)\n",
    "                pattern = r'^[+-]?\\d+\\.\\d+%?$'\n",
    "                if re.match(pattern, text):\n",
    "                    numeric_value = float(re.sub(r'[^\\d.-]', '', text))\n",
    "                    if -50 <= numeric_value <= 50:\n",
    "                        return numeric_value\n",
    "            \n",
    "            return None\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error scraping SG exports growth: {e}\")\n",
    "            return None\n",
    "\n",
    "    def scrape_sg_industrial_production(self):\n",
    "        try:\n",
    "            url = \"https://tradingeconomics.com/singapore/industrial-production\"\n",
    "            response = requests.get(url, headers=self.headers, timeout=10)\n",
    "            soup = BeautifulSoup(response.content, 'html.parser')\n",
    "            \n",
    "            for element in soup.find_all(['span', 'div', 'td']):\n",
    "                text = element.get_text(strip=True)\n",
    "                pattern = r'^[+-]?\\d+\\.\\d+%?$'\n",
    "                if re.match(pattern, text):\n",
    "                    numeric_value = float(re.sub(r'[^\\d.-]', '', text))\n",
    "                    if -50 <= numeric_value <= 50:\n",
    "                        return numeric_value\n",
    "            \n",
    "            return None\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error scraping SG industrial production: {e}\")\n",
    "            return None\n",
    "\n",
    "    def scrape_us_core_inflation_yoy(self):\n",
    "        try:\n",
    "            url = \"https://tradingeconomics.com/united-states/core-inflation-rate\"\n",
    "            response = requests.get(url, headers=self.headers, timeout=10)\n",
    "            soup = BeautifulSoup(response.content, 'html.parser')\n",
    "            \n",
    "            for element in soup.find_all(['span', 'div', 'td']):\n",
    "                text = element.get_text(strip=True)\n",
    "                if re.match(r'^\\d\\.\\d+%?$', text):\n",
    "                    numeric_value = float(re.sub(r'[^\\d.]', '', text))\n",
    "                    if 1 <= numeric_value <= 8:\n",
    "                        return numeric_value\n",
    "            \n",
    "            return None\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error scraping US core inflation: {e}\")\n",
    "            return None\n",
    "\n",
    "    def scrape_us_nonfarm_payrolls(self):\n",
    "        if self.fred_api_key:\n",
    "            fred_data = self.scrape_fred_data('PAYEMS')\n",
    "            if fred_data and fred_data['value']:\n",
    "                return fred_data['value']\n",
    "        \n",
    "        return None\n",
    "\n",
    "    def scrape_fed_funds_rate(self):\n",
    "        if self.fred_api_key:\n",
    "            fred_data = self.scrape_fred_data('FEDFUNDS')\n",
    "            if fred_data and fred_data['value']:\n",
    "                return fred_data['value']\n",
    "        \n",
    "        return None\n",
    "\n",
    "    def scrape_mas_policy_rate(self):\n",
    "        try:\n",
    "            return None\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error scraping MAS policy rate: {e}\")\n",
    "            return None\n",
    "\n",
    "    def get_all_singapore_bond_yields(self):\n",
    "        bond_data = {}\n",
    "        \n",
    "        bond_data['SG_1Y_yield'] = self.scrape_singapore_1y_bond()\n",
    "        time.sleep(1)\n",
    "        \n",
    "        bond_data['SG_2Y_yield'] = self.scrape_singapore_2y_bond()\n",
    "        time.sleep(1)\n",
    "        \n",
    "        bond_data['SG_5Y_yield'] = self.scrape_singapore_5y_bond()\n",
    "        time.sleep(1)\n",
    "        \n",
    "        bond_data['SG_10Y_yield'] = self.scrape_singapore_10y_bond()\n",
    "        time.sleep(1)\n",
    "        \n",
    "        return bond_data\n",
    "\n",
    "    def get_all_us_treasury_yields(self):\n",
    "        treasury_data = {}\n",
    "        \n",
    "        treasury_data['US_2Y_yield'] = self.scrape_us_2y_treasury()\n",
    "        time.sleep(1)\n",
    "        \n",
    "        treasury_data['US_5Y_yield'] = self.scrape_us_5y_treasury()\n",
    "        time.sleep(1)\n",
    "        \n",
    "        treasury_data['US_10Y_yield'] = self.scrape_us_10y_treasury()\n",
    "        time.sleep(1)\n",
    "        \n",
    "        treasury_data['US_30Y_yield'] = self.scrape_us_30y_treasury()\n",
    "        time.sleep(1)\n",
    "        \n",
    "        return treasury_data\n",
    "\n",
    "    def get_all_market_indicators(self):\n",
    "        indicators = {}\n",
    "        \n",
    "        indicators['VIX'] = self.scrape_vix()\n",
    "        indicators['DXY'] = self.scrape_dxy()\n",
    "        indicators['Gold'] = self.scrape_gold_price()\n",
    "        indicators['Oil_WTI'] = self.scrape_oil_wti_price()\n",
    "        indicators['MSCI_Asia_Pacific'] = self.scrape_msci_asia_pacific()\n",
    "        \n",
    "        indicators['daily_change_pct'] = self.scrape_sgd_usd_daily_change_pct()\n",
    "        indicators['sp500_change_pct'] = self.scrape_sp500_change_pct()\n",
    "        indicators['gold_change_pct'] = self.scrape_gold_change_pct()\n",
    "        indicators['singapore_sti_change_pct'] = self.scrape_singapore_sti_change_pct()\n",
    "        indicators['nikkei_change_pct'] = self.scrape_nikkei_change_pct()\n",
    "        indicators['hang_seng_change_pct'] = self.scrape_hang_seng_change_pct()\n",
    "        indicators['oil_wti_change_pct'] = self.scrape_oil_wti_change_pct()\n",
    "        indicators['dbs_change_pct'] = self.scrape_dbs_change_pct()\n",
    "        \n",
    "        indicators['high_52w'] = self.scrape_sgd_usd_52w_high()\n",
    "        indicators['low_52w'] = self.scrape_sgd_usd_52w_low()\n",
    "        \n",
    "        return indicators\n",
    "\n",
    "    def get_all_fred_economic_data(self):\n",
    "        if not self.fred_api_key:\n",
    "            return {}\n",
    "        \n",
    "        fred_data = {}\n",
    "        \n",
    "        fred_series = {\n",
    "            'US_GDP_GROWTH_YOY': 'A191RL1Q225SBEA',\n",
    "            'US_INFLATION_YOY': 'FPCPITOTLZGUSA',\n",
    "            'US_GDP': 'GDP',\n",
    "            'US_CPI': 'CPIAUCSL',\n",
    "            'US_UNEMPLOYMENT': 'UNRATE',\n",
    "            'EFFECTIVE_FED_RATE': 'FEDFUNDS',\n",
    "        }\n",
    "        \n",
    "        for name, series_id in fred_series.items():\n",
    "            data = self.scrape_fred_data(series_id)\n",
    "            if data:\n",
    "                fred_data[name] = data['value']\n",
    "                fred_data[f'{name}_date'] = data['date']\n",
    "            else:\n",
    "                fred_data[name] = None\n",
    "        \n",
    "        fred_data['SG_GDP_GROWTH_YOY'] = self.scrape_sg_gdp_from_trading_economics()\n",
    "        fred_data['SG_INFLATION_YOY'] = self.scrape_sg_inflation_from_trading_economics()\n",
    "        \n",
    "        return fred_data\n",
    "\n",
    "    def get_all_high_priority_data(self):\n",
    "        print(\"collecting high priority indicators\")\n",
    "        all_data = {}\n",
    "        \n",
    "        all_data['weekly_change_pct'] = self.scrape_sgd_usd_weekly_change_pct()\n",
    "        all_data['monthly_change_pct'] = self.scrape_sgd_usd_monthly_change_pct()\n",
    "        \n",
    "        all_data['volatility_20d'] = self.scrape_sgd_usd_volatility_20d()\n",
    "        \n",
    "        all_data['usd_5y_yield'] = self.scrape_us_5y_treasury()\n",
    "        \n",
    "        all_data['eur_usd_change_pct'] = self.scrape_eur_usd_change_pct()\n",
    "        all_data['usd_jpy_change_pct'] = self.scrape_usd_jpy_change_pct()\n",
    "        all_data['vix_change_pct'] = self.scrape_vix_change_pct()\n",
    "        all_data['dxy_change_pct'] = self.scrape_dxy_change_pct()\n",
    "        \n",
    "        all_data['sg_unemployment_rate'] = self.scrape_sg_unemployment_rate()\n",
    "        all_data['sg_exports_growth'] = self.scrape_sg_exports_growth()\n",
    "        \n",
    "        all_data['uob_change_pct'] = self.scrape_uob_change_pct()\n",
    "        all_data['dbs_change_pct'] = self.scrape_dbs_change_pct()\n",
    "        all_data['sg_reit_change_pct'] = self.scrape_sg_reit_change_pct()\n",
    "        \n",
    "        all_data['timestamp'] = datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "        \n",
    "        return all_data\n",
    "\n",
    "    def get_all_medium_priority_data(self):\n",
    "        print(\"collecting medium priority indicators\")\n",
    "        all_data = {}\n",
    "        \n",
    "        print(\"collecting market levels\")\n",
    "        all_data['sp500_level'] = self.scrape_sp500_level()\n",
    "        all_data['vix_level'] = self.scrape_vix_level()\n",
    "        all_data['dxy_level'] = self.scrape_dxy_level()\n",
    "        all_data['nikkei_level'] = self.scrape_nikkei_level()\n",
    "        all_data['hang_seng_level'] = self.scrape_hang_seng_level()\n",
    "        all_data['singapore_sti_level'] = self.scrape_singapore_sti_level()\n",
    "        all_data['gold_price'] = self.scrape_gold_price()\n",
    "        all_data['oil_wti_price'] = self.scrape_oil_wti_price()\n",
    "        \n",
    "        print(\"collecting currency rates\")\n",
    "        all_data['eur_usd_rate'] = self.scrape_eur_usd_rate()\n",
    "        all_data['usd_jpy_rate'] = self.scrape_usd_jpy_rate()\n",
    "        \n",
    "        print(\"collecting Singapore stock prices\")\n",
    "        all_data['dbs_stock_price'] = self.scrape_dbs_stock_price()\n",
    "        all_data['uob_stock_price'] = self.scrape_uob_stock_price()\n",
    "        all_data['sg_reit_index'] = self.scrape_sg_reit_index()\n",
    "        \n",
    "        print(\"collecting commodity indicators\")\n",
    "        all_data['baltic_dry_index'] = self.scrape_baltic_dry_index()\n",
    "        all_data['palm_oil_price'] = self.scrape_palm_oil_price()\n",
    "        all_data['baltic_dry_change_pct'] = self.scrape_baltic_dry_change_pct()\n",
    "        all_data['palm_oil_change_pct'] = self.scrape_palm_oil_change_pct()\n",
    "        \n",
    "        print(\"collecting economic indicators\")\n",
    "        all_data['sg_industrial_production'] = self.scrape_sg_industrial_production()\n",
    "        all_data['us_core_inflation_yoy'] = self.scrape_us_core_inflation_yoy()\n",
    "        all_data['us_nonfarm_payrolls'] = self.scrape_us_nonfarm_payrolls()\n",
    "        all_data['fed_funds_rate'] = self.scrape_fed_funds_rate()\n",
    "        all_data['mas_policy_rate'] = self.scrape_mas_policy_rate()\n",
    "        \n",
    "        all_data['timestamp'] = datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "        \n",
    "        return all_data\n",
    "\n",
    "    def collect_all_market_data(self):\n",
    "        all_data = {}\n",
    "        \n",
    "        forex_data = self.get_comprehensive_forex_data()\n",
    "        all_data.update(forex_data)\n",
    "        \n",
    "        sg_bonds = self.get_all_singapore_bond_yields()\n",
    "        all_data.update(sg_bonds)\n",
    "        \n",
    "        us_bonds = self.get_all_us_treasury_yields()\n",
    "        all_data.update(us_bonds)\n",
    "        \n",
    "        market_indicators = self.get_all_market_indicators()\n",
    "        all_data.update(market_indicators)\n",
    "        \n",
    "        fred_data = self.get_all_fred_economic_data()\n",
    "        all_data.update(fred_data)\n",
    "        \n",
    "        high_priority = self.get_all_high_priority_data()\n",
    "        all_data.update(high_priority)\n",
    "        \n",
    "        medium_priority = self.get_all_medium_priority_data()\n",
    "        all_data.update(medium_priority)\n",
    "        \n",
    "        all_data['timestamp'] = datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "        \n",
    "        return all_data\n",
    "\n",
    "    def test_time_period_changes(self):\n",
    "        \n",
    "        weekly_result = self.scrape_sgd_usd_weekly_change_pct()\n",
    "\n",
    "        monthly_result = self.scrape_sgd_usd_monthly_change_pct()\n",
    "\n",
    "        success_count = sum([1 for x in [weekly_result, monthly_result] if x is not None])\n",
    "\n",
    "        return weekly_result, monthly_result\n",
    "\n",
    "    def test_marketwatch_scrapers(self):\n",
    "\n",
    "        uob_change = self.scrape_uob_change_pct()\n",
    "        print(f\"UOB Change: {uob_change:+.2f}%\" if uob_change is not None else \"UOB Change: N/A\")\n",
    "        \n",
    "        dbs_change = self.scrape_dbs_change_pct()\n",
    "        print(f\"DBS Change: {dbs_change:+.2f}%\" if dbs_change is not None else \"DBS Change: N/A\")\n",
    "        \n",
    "        success_count = sum([1 for x in [uob_change, dbs_change] if x is not None])\n",
    "        return uob_change, dbs_change\n",
    "\n",
    "    def print_simple_results(self, data):\n",
    "        sgd_usd = data.get('current_rate')\n",
    "        if sgd_usd:\n",
    "            usd_sgd = 1 / sgd_usd\n",
    "            print(f\"SGDUSD rate: {sgd_usd:.4f}\")\n",
    "            print(f\"USDSGD rate: {usd_sgd:.4f}\")\n",
    "        else:\n",
    "            print(\"SGDUSD rate: N/A\")\n",
    "            print(\"USDSGD rate: N/A\")\n",
    "        \n",
    "        daily_change = data.get('daily_change_pct')\n",
    "        if daily_change is not None:\n",
    "            print(f\"daily_change_pct: {daily_change:+.2f}\")\n",
    "        else:\n",
    "            print(\"daily_change_pct: N/A\")\n",
    "        \n",
    "        high_52w = data.get('high_52w')\n",
    "        print(f\"high_52w: {high_52w:.4f}\" if high_52w else \"high_52w: N/A\")\n",
    "            \n",
    "        low_52w = data.get('low_52w')\n",
    "        print(f\"low_52w: {low_52w:.4f}\" if low_52w else \"low_52w: N/A\")\n",
    "        \n",
    "        bond_mapping = {\n",
    "            'SG_1Y_yield': 'SG1Y',\n",
    "            'SG_2Y_yield': 'SG2Y',\n",
    "            'SG_5Y_yield': 'SG5Y', \n",
    "            'SG_10Y_yield': 'SG10Y',\n",
    "            'US_2Y_yield': 'US2Y',\n",
    "            'US_5Y_yield': 'US5Y',\n",
    "            'US_10Y_yield': 'US10Y',\n",
    "            'US_30Y_yield': 'US30Y'\n",
    "        }\n",
    "        \n",
    "        for key, label in bond_mapping.items():\n",
    "            value = data.get(key)\n",
    "            print(f\"{label}: {value:.3f}\" if value else f\"{label}: N/A\")\n",
    "        \n",
    "        indicator_mapping = {\n",
    "            'VIX': 'VIX',\n",
    "            'DXY': 'DXY',\n",
    "            'Gold': 'Gold',\n",
    "            'Oil_WTI': 'Oil',\n",
    "            'MSCI_Asia_Pacific': 'MSCI_Asia'\n",
    "        }\n",
    "        \n",
    "        for key, label in indicator_mapping.items():\n",
    "            value = data.get(key)\n",
    "            print(f\"{label}: {value:.2f}\" if value else f\"{label}: N/A\")\n",
    "        \n",
    "        change_mapping = {\n",
    "            'sp500_change_pct': 'sp500_change_pct',\n",
    "            'gold_change_pct': 'gold_change_pct',\n",
    "            'singapore_sti_change_pct': 'singapore_sti_change_pct',\n",
    "            'nikkei_change_pct': 'nikkei_change_pct',\n",
    "            'hang_seng_change_pct': 'hang_seng_change_pct',\n",
    "            'oil_wti_change_pct': 'oil_wti_change_pct',\n",
    "            'dbs_change_pct': 'dbs_change_pct'\n",
    "        }\n",
    "        \n",
    "        for key, label in change_mapping.items():\n",
    "            value = data.get(key)\n",
    "            print(f\"{label}: {value:+.2f}\" if value is not None else f\"{label}: N/A\")\n",
    "        \n",
    "        economic_mapping = {\n",
    "            'SG_GDP_GROWTH_YOY': 'sg_gdp_growth_yoy',\n",
    "            'US_GDP_GROWTH_YOY': 'us_gdp_growth_yoy',\n",
    "            'SG_INFLATION_YOY': 'sg_inflation_yoy',\n",
    "            'US_INFLATION_YOY': 'us_inflation_yoy'\n",
    "        }\n",
    "        \n",
    "        for key, label in economic_mapping.items():\n",
    "            value = data.get(key)\n",
    "            print(f\"{label}: {value:.2f}\" if value is not None else f\"{label}: N/A\")\n",
    "\n",
    "    def print_high_priority_results(self, data):\n",
    "\n",
    "        weekly_change = data.get('weekly_change_pct')\n",
    "        print(f\"weekly_change_pct: {weekly_change:+.2f}\" if weekly_change is not None else \"weekly_change_pct: N/A\")\n",
    "        \n",
    "        monthly_change = data.get('monthly_change_pct')\n",
    "        print(f\"monthly_change_pct: {monthly_change:+.2f}\" if monthly_change is not None else \"monthly_change_pct: N/A\")\n",
    "        \n",
    "        usd_5y = data.get('usd_5y_yield')\n",
    "        print(f\"usd_5y_yield: {usd_5y:.3f}\" if usd_5y is not None else \"usd_5y_yield: N/A\")\n",
    "        \n",
    "        forex_mapping = {\n",
    "            'eur_usd_change_pct': 'eur_usd_change_pct',\n",
    "            'usd_jpy_change_pct': 'usd_jpy_change_pct',\n",
    "            'vix_change_pct': 'vix_change_pct',\n",
    "            'dxy_change_pct': 'dxy_change_pct'\n",
    "        }\n",
    "        \n",
    "        for key, label in forex_mapping.items():\n",
    "            value = data.get(key)\n",
    "            print(f\"{label}: {value:+.2f}\" if value is not None else f\"{label}: N/A\")\n",
    "        \n",
    "        sg_unemployment = data.get('sg_unemployment_rate')\n",
    "        print(f\"sg_unemployment_rate: {sg_unemployment:.2f}\" if sg_unemployment is not None else \"sg_unemployment_rate: N/A\")\n",
    "        \n",
    "        sg_exports = data.get('sg_exports_growth')\n",
    "        print(f\"sg_exports_growth: {sg_exports:+.2f}\" if sg_exports is not None else \"sg_exports_growth: N/A\")\n",
    "        \n",
    "        uob_change = data.get('uob_change_pct')\n",
    "        print(f\"uob_change_pct: {uob_change:+.2f}\" if uob_change is not None else \"uob_change_pct: N/A\")\n",
    "        \n",
    "        dbs_change = data.get('dbs_change_pct')\n",
    "        print(f\"dbs_change_pct: {dbs_change:+.2f}\" if dbs_change is not None else \"dbs_change_pct: N/A\")\n",
    "        \n",
    "        sg_reit = data.get('sg_reit_change_pct')\n",
    "        print(f\"sg_reit_change_pct: {sg_reit:+.2f}\" if sg_reit is not None else \"sg_reit_change_pct: N/A\")\n",
    "\n",
    "    def print_medium_priority_results(self, data):\n",
    "        market_levels = {\n",
    "            'sp500_level': 'sp500_level',\n",
    "            'vix_level': 'vix_level',\n",
    "            'dxy_level': 'dxy_level',\n",
    "            'nikkei_level': 'nikkei_level',\n",
    "            'hang_seng_level': 'hang_seng_level',\n",
    "            'singapore_sti_level': 'singapore_sti_level',\n",
    "            'gold_price': 'gold_price',\n",
    "            'oil_wti_price': 'oil_wti_price'\n",
    "        }\n",
    "        \n",
    "        for key, label in market_levels.items():\n",
    "            value = data.get(key)\n",
    "            print(f\"{label}: {value:.2f}\" if value is not None else f\"{label}: N/A\")\n",
    "        \n",
    "        currency_rates = {\n",
    "            'eur_usd_rate': 'eur_usd_rate',\n",
    "            'usd_jpy_rate': 'usd_jpy_rate'\n",
    "        }\n",
    "        \n",
    "        for key, label in currency_rates.items():\n",
    "            value = data.get(key)\n",
    "            print(f\"{label}: {value:.4f}\" if value is not None else f\"{label}: N/A\")\n",
    "        \n",
    "        sg_stocks = {\n",
    "            'dbs_stock_price': 'dbs_stock_price',\n",
    "            'uob_stock_price': 'uob_stock_price',\n",
    "            'sg_reit_index': 'sg_reit_index'\n",
    "        }\n",
    "        \n",
    "        for key, label in sg_stocks.items():\n",
    "            value = data.get(key)\n",
    "            print(f\"{label}: {value:.2f}\" if value is not None else f\"{label}: N/A\")\n",
    "        \n",
    "        commodities = {\n",
    "            'baltic_dry_index': 'baltic_dry_index',\n",
    "            'palm_oil_price': 'palm_oil_price',\n",
    "            'baltic_dry_change_pct': 'baltic_dry_change_pct',\n",
    "            'palm_oil_change_pct': 'palm_oil_change_pct'\n",
    "        }\n",
    "        \n",
    "        for key, label in commodities.items():\n",
    "            value = data.get(key)\n",
    "            if 'change_pct' in key:\n",
    "                print(f\"{label}: {value:+.2f}\" if value is not None else f\"{label}: N/A\")\n",
    "            else:\n",
    "                print(f\"{label}: {value:.2f}\" if value is not None else f\"{label}: N/A\")\n",
    "        \n",
    "        economic = {\n",
    "            'sg_industrial_production': 'sg_industrial_production',\n",
    "            'us_core_inflation_yoy': 'us_core_inflation_yoy',\n",
    "            'us_nonfarm_payrolls': 'us_nonfarm_payrolls',\n",
    "            'fed_funds_rate': 'fed_funds_rate',\n",
    "            'mas_policy_rate': 'mas_policy_rate'\n",
    "        }\n",
    "        \n",
    "        for key, label in economic.items():\n",
    "            value = data.get(key)\n",
    "            if 'payrolls' in key:\n",
    "                print(f\"{label}: {value:.0f}\" if value is not None else f\"{label}: N/A\")\n",
    "            else:\n",
    "                print(f\"{label}: {value:.2f}\" if value is not None else f\"{label}: N/A\")\n",
    "\n",
    "    def print_comprehensive_results(self, data):\n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(\"COMPREHENSIVE SGD/USD MARKET DATA\")\n",
    "        print(\"=\"*80)\n",
    "        \n",
    "        self.print_simple_results(data)\n",
    "        \n",
    "        self.print_high_priority_results(data)\n",
    "        \n",
    "        self.print_medium_priority_results(data)\n",
    "\n",
    "        successful_scrapers = sum([1 for v in data.values() if v is not None and v != \"N/A\" and v != data.get('timestamp')])\n",
    "        total_scrapers = len(data) - 1\n",
    "        \n",
    "        print(successful_scrapers)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    FRED_API_KEY = os.getenv('FRED_API_KEY')\n",
    "    \n",
    "    scrapers = ComprehensiveSGDUSDScrapers(fred_api_key=FRED_API_KEY)\n",
    "    \n",
    "    print(\"scraping\")\n",
    "    all_data = scrapers.collect_all_market_data()\n",
    "    \n",
    "    scrapers.print_comprehensive_results(all_data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
